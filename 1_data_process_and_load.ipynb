{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理模块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**目录：**\n",
    "1. 标签类别收集\n",
    "\n",
    "2. 训练样本读取\n",
    "\n",
    "2. 样本转化为符合BERT模型的特征\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTsg5j62xfnk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\威威的小荔枝\\Desktop\\第五课_代码\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 数据形式\n",
    "\n",
    "　　我们使用的atis数据集已经将训练集、验证集和测试集区分好\n",
    "- label文件保存了意图识别的标签\n",
    "- seq.in文件每行保存了一句输入样本\n",
    "- seq.out文件每行保存了样本的NER标签序列，以空格隔开(下节课)\n",
    "\n",
    "<img src=\"imgs/数据集结构.png\"  width=\"300\" height=\"300\" align=\"left\" />\n",
    "<img src=\"imgs/数据集_举例.png\"  width=\"600\" height=\"400\" align=\"right\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 标签集：将所有出现的意图标签统计出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1272,
     "status": "ok",
     "timestamp": 1586067414596,
     "user": {
      "displayName": "Rongfan Liao",
      "photoUrl": "",
      "userId": "07803922812103577726"
     },
     "user_tz": -480
    },
    "id": "b4eeR63Y6x1Y",
    "outputId": "50c61750-cd65-4da8-8b0c-454da44a9cd0"
   },
   "outputs": [],
   "source": [
    "def vocab_process(data_dir):\n",
    "    '''\n",
    "    args:\n",
    "        data_dir: 数据集所在的路径；\n",
    "    \n",
    "    return:\n",
    "        None\n",
    "    \n",
    "    results:\n",
    "        intent的label类型(写入一个txt文件);\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 标签集合输出到如下文件中\n",
    "    intent_label_vocab = 'intent_label.txt'\n",
    "\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    \n",
    "    # 收集intent标签\n",
    "    with open(os.path.join(train_dir, 'label'), 'r', encoding='utf-8') as f_r, open(os.path.join(data_dir, intent_label_vocab), 'w',\n",
    "                                                                                    encoding='utf-8') as f_w:\n",
    "        # 提取所有出现的intent的label类型\n",
    "        intent_vocab = set()\n",
    "        for line in f_r:\n",
    "            line = line.strip()\n",
    "            intent_vocab.add(line)\n",
    "        \n",
    "        # 因为训练集已经分好了，所以可能出现验证集中有而训练集中没有的label，以“UNK”来表示这种label; \n",
    "        # 后面读取dev集，就需要将未见过的intent标签归类为\"UNK\"\n",
    "        additional_tokens = [\"UNK\"]\n",
    "        for token in additional_tokens:\n",
    "            f_w.write(token + '\\n')\n",
    "        \n",
    "        # 将vocab以字典序排列\n",
    "        intent_vocab = sorted(list(intent_vocab))\n",
    "        for intent in intent_vocab:\n",
    "            f_w.write(intent + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_dir = \"./bert_finetune_cls/data/atis/\"\n",
    "vocab_process(atis_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后面标签就分别通过简单的读取函数就可以读取出来了\n",
    "\n",
    "def get_intent_labels(args):\n",
    "    return [label.strip() for label in open(os.path.join(args.data_dir, args.task, args.intent_label_file), 'r', encoding='utf-8')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 样本读取为样本实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for simple sequence classification. 一个单独的样本实例\n",
    "    一个样本完全可以用一个dict来表示，但是使用 InputExample 类，作为一个python类，具有一些方便之处\n",
    "\n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        words: list. The words of the sequence.\n",
    "        intent_label: (Optional) string. The intent label of the example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, intent_label=None):\n",
    "        self.guid = guid # 每个样本的独特的序号\n",
    "        self.words = words # 样本的输入序列\n",
    "        self.intent_label = intent_label #样本的intent标签\n",
    "\n",
    "    def __repr__(self):\n",
    "        # 默认为： “类名+object at+内存地址”这样的信息表示这个实例；\n",
    "        # 这里我们重写成了想要输出的信息；\n",
    "        # print(input_example) 时候显示；\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        \n",
    "        # __dict__： \n",
    "        # 类 的静态函数、类函数、普通函数、全局变量以及一些内置的属性都是放在类__dict__里的\n",
    "        # 对象实例的__dict__中存储了一些self.xxx的一些东西\n",
    "        # 参见 https://www.cnblogs.com/starrysky77/p/9102344.html\n",
    "        \n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        # 类的性质等信息dump进入json string\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClsProcessor(object):\n",
    "    \"\"\"\n",
    "    Processor for the BERT classfication data set \n",
    "    \n",
    "    JointBERT项目的数据处理器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        \n",
    "        # 读出我们已经整理好的意图标签；\n",
    "        self.intent_labels = get_intent_labels(args)\n",
    "        \n",
    "        # 每个数据集的文件夹里面，数据格式是一致的，文件名也一致；\n",
    "        self.input_text_file = 'seq.in'\n",
    "        self.intent_label_file = 'label'\n",
    "    \n",
    "    # 按行读取文件\n",
    "    @classmethod\n",
    "    def _read_file(cls, input_file):\n",
    "        \"\"\"\n",
    "        Reads a tab separated value file.\n",
    "        读一个文件，以行为单位，先把每行读出来，读字段是后续的事情；        \n",
    "        \n",
    "        \"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = []\n",
    "            for line in f:\n",
    "                lines.append(line.strip())\n",
    "            return lines\n",
    "\n",
    "    def _create_examples(self, texts, intents, set_type):\n",
    "        \"\"\"\n",
    "        Creates examples for the training and dev sets.        \n",
    "       \n",
    "        Args:\n",
    "            texts: list. Sequence of unsplitted texts. 需要处理的文本组成的列表\n",
    "            intents: list. Sequence of intent labels. 意图label组成的列表\n",
    "            set_type: str. train, dev, test. 训练集/开发集/测试集\n",
    "            \n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        for i, (text, intent) in enumerate(zip(texts, intents)):\n",
    "            guid = \"%s-%s\" % (set_type, i)   # 给每个样本一个编号\n",
    "            # 1. input_text\n",
    "            words = text.split()  \n",
    "            # 2. intent\n",
    "            #    如果不在已知的意图类别中，则归为\"UNK\"\n",
    "            intent_label = self.intent_labels.index(intent) if intent in self.intent_labels else self.intent_labels.index(\"UNK\")\n",
    "            \n",
    "            examples.append(InputExample(guid=guid, words=words, intent_label=intent_label, ))\n",
    "        return examples\n",
    "\n",
    "    def get_examples(self, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode: train, dev, test; 区分训练/开发/测试集\n",
    "        \"\"\"\n",
    "        data_path = os.path.join(self.args.data_dir, self.args.task, mode)\n",
    "        print(\"LOOKING AT {}\".format(data_path))\n",
    "        return self._create_examples(texts=self._read_file(os.path.join(data_path, self.input_text_file)),\n",
    "                                     intents=self._read_file(os.path.join(data_path, self.intent_label_file)),\n",
    "                                     set_type=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先构建参数\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# 实际使用应该是命令行传入的参数，不过我这里直接赋值传入\n",
    "# parser.add_argument(\"--task\", default=None, required=True, type=str, help=\"The name of the task to train\")\n",
    "# parser.add_argument(\"--data_dir\", default=\"./data\", type=str, help=\"The input data dir\")\n",
    "# parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "class Args():\n",
    "    task =  None\n",
    "    data_dir =  None\n",
    "    intent_label_file =  None\n",
    "\n",
    "args = Args()\n",
    "args.task = \"atis\"\n",
    "args.data_dir = \"./bert_finetune_cls/data/\"\n",
    "args.intent_label_file = \"intent_label.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化\n",
    "\n",
    "processor = ClsProcessor(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNK', 'atis_abbreviation', 'atis_aircraft', 'atis_aircraft#atis_flight#atis_flight_no', 'atis_airfare', 'atis_airline', 'atis_airline#atis_flight_no', 'atis_airport', 'atis_capacity', 'atis_cheapest', 'atis_city', 'atis_distance', 'atis_flight', 'atis_flight#atis_airfare', 'atis_flight_no', 'atis_flight_time', 'atis_ground_fare', 'atis_ground_service', 'atis_ground_service#atis_ground_fare', 'atis_meal', 'atis_quantity', 'atis_restriction']\n"
     ]
    }
   ],
   "source": [
    "# 看一下processor的属性\n",
    "\n",
    "print(processor.intent_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOKING AT ./bert_finetune_cls/data/atis\\train\n",
      "4478\n",
      "{\n",
      "  \"guid\": \"train-0\",\n",
      "  \"intent_label\": 12,\n",
      "  \"words\": [\n",
      "    \"i\",\n",
      "    \"want\",\n",
      "    \"to\",\n",
      "    \"fly\",\n",
      "    \"from\",\n",
      "    \"baltimore\",\n",
      "    \"to\",\n",
      "    \"dallas\",\n",
      "    \"round\",\n",
      "    \"trip\"\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 读取train样本\n",
    "train_examples = processor.get_examples(\"train\")\n",
    "print(len(train_examples))\n",
    "print(train_examples[0])  # InputExample __repr__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果有多个数据集，则数据集的processor可以通过映射得到\n",
    "\n",
    "processors = {\n",
    "    \"atis\": ClsProcessor,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 将数据处理成可以喂给模型的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, intent_label_id):\n",
    "        self.input_ids = input_ids  # 输入样本序列在bert词表里的索引，可以直接喂给nn.embedding\n",
    "        self.attention_mask = attention_mask  # 注意力mask，padding的部分为0，其他为1\n",
    "        self.token_type_ids = token_type_ids  # 表示每个token属于句子1还是句子2\n",
    "        self.intent_label_id = intent_label_id  \n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, \n",
    "                                 max_seq_len, \n",
    "                                 tokenizer,\n",
    "                                 pad_token_label_id=-100,  \n",
    "                                 cls_token_segment_id=0,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    \"\"\"\n",
    "    将之前读取的数据进行添加[CLS],[SEP]标记，padding等操作\n",
    "    \n",
    "    args:\n",
    "        examples: 样本实例列表\n",
    "        pad_token_label_id: Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "        cls_token_segment_id： 取0\n",
    "        sequence_a_segment_id： 取0\n",
    "        pad_token_segment_id： 取0\n",
    "        mask_padding_with_zero： attention mask;\n",
    "    \n",
    "    \"\"\"\n",
    "    # Setting based on the current model type 这里我们以BERT tokenizer为例讲解\n",
    "    cls_token = tokenizer.cls_token   # [CLS]\n",
    "    sep_token = tokenizer.sep_token   # [SEP]\n",
    "    unk_token = tokenizer.unk_token   # [UNK]\n",
    "    pad_token_id = tokenizer.pad_token_id  # [PAD]编号为0\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 1000 == 0:\n",
    "            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        # Tokenize words\n",
    "        tokens = []\n",
    "        for w in example.words:\n",
    "            toks = tokenizer.tokenize(w)\n",
    "            tokens.extend(toks)\n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        # 如果句子长了就截断\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "        \n",
    "        # 把tokens转化为ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        \n",
    "        # check长度是否符合\n",
    "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_ids), max_seq_len)\n",
    "        intent_label_id = int(example.intent_label)\n",
    "\n",
    "        if ex_index < 5:\n",
    "            print(\"*** Example ***\")\n",
    "            print(\"guid: %s\" % example.guid)\n",
    "            print(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            print(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
    "            print(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
    "            print(\"intent_label: %s (id = %d)\" % (example.intent_label, intent_label_id))\n",
    "        \n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids,\n",
    "                          intent_label_id=intent_label_id,\n",
    "                          ))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, mode):\n",
    "    processor = processors[args.task](args)\n",
    "\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        'cached_{}_{}_{}_{}'.format(\n",
    "            mode,\n",
    "            args.task,\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            args.max_seq_len\n",
    "        )\n",
    "    )\n",
    "    print(cached_features_file)\n",
    "\n",
    "    if os.path.exists(cached_features_file):\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        # Load data features from dataset file\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        if mode == \"train\":\n",
    "            examples = processor.get_examples(\"train\")\n",
    "        elif mode == \"dev\":\n",
    "            examples = processor.get_examples(\"dev\")\n",
    "        elif mode == \"test\":\n",
    "            examples = processor.get_examples(\"test\")\n",
    "        else:\n",
    "            raise Exception(\"For mode, Only train, dev, test is available\")\n",
    "\n",
    "        features = convert_examples_to_features(examples, \n",
    "                                                args.max_seq_len,\n",
    "                                                tokenizer,)\n",
    "        print(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    # Convert to Tensors and build dataset 将特征转化为tensor\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_intent_label_ids = torch.tensor([f.intent_label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    # 将各种tensor打包，类似zip，要求各 tensor 第一维相等\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids,\n",
    "        all_attention_mask,\n",
    "        all_token_type_ids,\n",
    "        all_intent_label_ids,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这一步涉及到不同模型的tokenizer\n",
    "\n",
    "from transformers import BertConfig\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from bert_finetune_cls.model import ClsBERT\n",
    "\n",
    "from bert_finetune_cls.trainer import Trainer\n",
    "from bert_finetune_cls.utils import init_logger, load_tokenizer, read_prediction_text, set_seed, MODEL_CLASSES, MODEL_PATH_MAP\n",
    "from bert_finetune_cls.data_loader import load_and_cache_examples\n",
    "\n",
    "\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, ClsBERT, BertTokenizer),\n",
    "}\n",
    "\n",
    "MODEL_PATH_MAP = {\n",
    "    'bert': './bert_finetune_cls/resources/uncased_L-2_H-128_A-2',\n",
    "}\n",
    "\n",
    "def load_tokenizer(args):\n",
    "    return MODEL_CLASSES[args.model_type][2].from_pretrained(args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x21cdfc6b860>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先构建参数\n",
    "class Args():\n",
    "    task =  None\n",
    "    data_dir =  None\n",
    "    intent_label_file =  None\n",
    "\n",
    "args = Args()\n",
    "args.task = \"atis\"\n",
    "args.data_dir = \"./bert_finetune_cls/data\"\n",
    "args.intent_label_file = \"intent_label.txt\"\n",
    "args.max_seq_len = 50\n",
    "args.model_type = \"bert\"\n",
    "args.model_dir = \"bert_finetune_cls/experiments/outputs/clsbert_0\"\n",
    "args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n",
    "\n",
    "args.train_batch_size = 4\n",
    "\n",
    "tokenizer = load_tokenizer(args)\n",
    "load_and_cache_examples(args, tokenizer, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 pytorch的dataloader加载函数\n",
    "\n",
    "这里是pytorch dataload 的pipeline固定写法,同学们需要了解清楚执行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2265,  2033,  1996,  7599,  2013,  2035, 13586,  2000,  2293,\n",
      "          2492,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  3531,  2265,  2033,  2035, 27092,  2005,  7599,  2013,  7573,\n",
      "          2000,  9182,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2598,  5193,  2899,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2054,  7599,  2079,  2017,  2031,  1999,  1996,  2851,  1997,\n",
      "          2244,  9086,  2006,  2142,  7608,  2013,  6278,  2000,  2624,  3799,\n",
      "          1998,  1037,  2644,  7840,  1999,  7573,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]) torch.Size([4, 50])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]) torch.Size([4, 50])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]) torch.Size([4, 50])\n",
      "tensor([12,  4, 17, 12]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(args)\n",
    "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "# dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\")\n",
    "# test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "\n",
    "# torch自带的sampler类，功能是每次返回一个随机的样本索引\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "# 使用dataloader输出batch\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "device = \"cpu\"\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch) # 将batch上传到显卡\n",
    "    inputs = {\"input_ids\": batch[0],\n",
    "              \"attention_mask\": batch[1],\n",
    "              \"token_type_ids\": batch[2],\n",
    "              \"intent_label_ids\": batch[3],}\n",
    "    \n",
    "    if step == 0:\n",
    "        print(inputs[\"input_ids\"], inputs[\"input_ids\"].shape)\n",
    "        print(inputs[\"attention_mask\"], inputs[\"attention_mask\"].shape)\n",
    "        print(inputs[\"token_type_ids\"], inputs[\"token_type_ids\"].shape)\n",
    "        print(inputs[\"intent_label_ids\"], inputs[\"intent_label_ids\"].shape)\n",
    "    \n",
    "    # 输入模型的形式\n",
    "    # outputs = self.model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
