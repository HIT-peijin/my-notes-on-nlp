{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建与损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**目录：**\n",
    "1. BERT分类模型\n",
    "\n",
    "2. 损失函数计算\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\威威的小荔枝\\Desktop\\第五课_代码\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "\n",
    "# 以BERT为预训练模型进行讲解\n",
    "from transformers import BertPreTrainedModel, BertModel, BertConfig\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 意图分类任务的MLP层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent分类的MLP全连接层\n",
    "class IntentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_intent_labels, dropout_rate=0.):\n",
    "        super(IntentClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_intent_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_dim]\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主要的模型框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClsBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, args, intent_label_lst):\n",
    "        super(ClsBERT, self).__init__(config)\n",
    "        self.args = args\n",
    "        self.num_intent_labels = len(intent_label_lst)\n",
    "        self.bert = BertModel(config=config)  # Load pretrained bert\n",
    "\n",
    "        self.intent_classifier = IntentClassifier(config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, intent_label_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "\n",
    "        outputs = ((intent_logits),) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        # 1. Intent Softmax\n",
    "        if intent_label_ids is not None:\n",
    "            if self.num_intent_labels == 1:\n",
    "                intent_loss_fct = nn.MSELoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1), intent_label_ids.view(-1))\n",
    "            else:\n",
    "                intent_loss_fct = nn.CrossEntropyLoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels), intent_label_ids.view(-1))\n",
    "\n",
    "            outputs = (intent_loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数 CrossEntropyLoss\n",
    "Pytorch中CrossEntropyLoss()函数的主要是将softmax -> log -> NLLLoss合并到一块得到的结果， 所以我们自己不需要求softmax。\n",
    "$$L=- \\sum_{i=1}^{N}y_i* \\log \\hat{y_i}$$\n",
    "$y_i$是真正类别的one-hot分布，只有真实类别的概率为1，其他都是0，$\\hat{y_i}$是经由softmax后的分布\n",
    "\n",
    "- softmax将输出数据规范化为一个概率分布。\n",
    "\n",
    "- 然后将Softmax之后的结果取log\n",
    "\n",
    "- 输入负对数损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 举例查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, DistilBertConfig, AlbertConfig\n",
    "from transformers import BertTokenizer, DistilBertTokenizer, AlbertTokenizer\n",
    "\n",
    "from bert_finetune_cls.model import ClsBERT\n",
    "from bert_finetune_cls.utils import init_logger, load_tokenizer, get_intent_labels\n",
    "from bert_finetune_cls.data_loader import load_and_cache_examples\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, ClsBERT, BertTokenizer),\n",
    "}\n",
    "\n",
    "MODEL_PATH_MAP = {\n",
    "    'bert': './bert_finetune_cls/resources/uncased_L-2_H-128_A-2',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先构建参数\n",
    "class Args():\n",
    "    task =  None\n",
    "    data_dir =  None\n",
    "    intent_label_file =  None\n",
    "\n",
    "\n",
    "args = Args()\n",
    "args.task = \"atis\"\n",
    "args.data_dir = \"./bert_finetune_cls/data\"\n",
    "args.intent_label_file = \"intent_label.txt\"\n",
    "args.max_seq_len = 50\n",
    "args.model_type = \"bert\"\n",
    "args.model_dir = \"bert_finetune_cls/experiments/outputs/clsbert_0\"\n",
    "args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n",
    "\n",
    "args.train_batch_size = 4\n",
    "args.dropout_rate = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(args)\n",
    "config = MODEL_CLASSES[args.model_type][0].from_pretrained(args.model_name_or_path)\n",
    "\n",
    "intent_label_lst = get_intent_labels(args)\n",
    "\n",
    "model = ClsBERT(config, args, intent_label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  tensor([[ 101, 1045, 2342, 1037, 3462, 2006, 2250, 2710, 2013, 4361, 2000, 2624,\n",
      "         5277, 2007, 1037, 3913, 7840, 1999, 5887,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 101, 2425, 2033, 2055, 1996, 2598, 5193, 1999, 3190,  102,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 101, 1045, 2215, 2035, 7599, 2013, 5865, 2000, 2899, 5887, 2006, 9432,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 101, 1045, 2052, 2066, 1037, 3462, 2090, 3731, 1998, 5865, 2006, 2151,\n",
      "         2154, 2012, 2028, 1999, 1996, 5027,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]])\n",
      "intent_logits:  tensor([[-0.0428,  0.1462,  0.1511, -0.3988,  0.0387, -0.1444, -0.0829,  0.1043,\n",
      "          0.1997, -0.0519,  0.1182,  0.1451, -0.1335,  0.1733,  0.0796, -0.1543,\n",
      "          0.1181,  0.0710,  0.0997, -0.0461,  0.2913, -0.0018],\n",
      "        [ 0.1034, -0.0016, -0.0025, -0.3392,  0.0346, -0.0673, -0.1421, -0.0102,\n",
      "          0.0724, -0.1024,  0.1844,  0.2055, -0.0678,  0.0982,  0.2280, -0.1388,\n",
      "          0.0932, -0.0245, -0.0779, -0.0694,  0.3494, -0.0674],\n",
      "        [ 0.2469,  0.1237,  0.0440, -0.4105, -0.0475, -0.1562, -0.1326,  0.0093,\n",
      "          0.1365, -0.0458,  0.1092,  0.2410,  0.0194,  0.1575,  0.0537, -0.1394,\n",
      "          0.0427,  0.1720,  0.0906, -0.0336,  0.3591, -0.0675],\n",
      "        [ 0.0426, -0.0024, -0.0127, -0.2843, -0.0175, -0.1991, -0.1655,  0.1058,\n",
      "          0.0810, -0.1889,  0.2861,  0.2333, -0.0199,  0.1531,  0.1280, -0.0108,\n",
      "         -0.0102,  0.1368,  0.0010, -0.0302,  0.3278, -0.0891]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "intent_logits:  torch.Size([4, 22])\n",
      "intent_loss:  tensor(3.1669, grad_fn=<NllLossBackward>)\n",
      "input_ids:  tensor([[  101,  2054,  2024,  1996, 12635,  2482,  6165,  1999,  2624,  3799,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2265,  2033,  2035,  7599,  2013,  7573,  2000,  6278,  2029,\n",
      "          3710,  1037,  7954,  2005,  1996,  2154,  2044,  4826,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2054,  2024,  1996,  7599,  2013, 22954,  2000,  2624,  4560,\n",
      "          2036,  2006,  9857,  1996, 12965,  1997,  2089,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  2342,  2000,  2175,  2000,  2624,  5277,  2013,  4361,\n",
      "          2021,  1045,  2215,  2000,  2644,  7840,  1999,  7573,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "intent_logits:  tensor([[ 0.0307,  0.0530,  0.0435, -0.3263,  0.0102, -0.1475, -0.1463, -0.0301,\n",
      "          0.2136, -0.1400,  0.2092,  0.1866, -0.0745,  0.1833,  0.0628, -0.1538,\n",
      "          0.0999,  0.1591,  0.1233, -0.0617,  0.4094, -0.0411],\n",
      "        [ 0.0767,  0.0314,  0.0575, -0.4364, -0.0981, -0.1639, -0.2027, -0.0446,\n",
      "          0.2210, -0.1573,  0.1822,  0.2126,  0.0597,  0.0573,  0.0417, -0.0800,\n",
      "          0.0920,  0.1917,  0.0776, -0.0086,  0.3370, -0.1895],\n",
      "        [ 0.0242,  0.0609,  0.0908, -0.3957,  0.0824,  0.0399, -0.0985,  0.0975,\n",
      "          0.0818, -0.0520,  0.1377,  0.2284,  0.0749,  0.1952,  0.0500, -0.1970,\n",
      "          0.1331,  0.0991, -0.0058,  0.0062,  0.2605, -0.0890],\n",
      "        [ 0.1163, -0.0057,  0.1425, -0.4489,  0.0849, -0.2412, -0.1547, -0.0080,\n",
      "          0.1790, -0.0444,  0.1745,  0.1499, -0.1609,  0.1742,  0.0744, -0.1377,\n",
      "          0.0712,  0.0673,  0.1315, -0.0093,  0.3089, -0.0943]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "intent_logits:  torch.Size([4, 22])\n",
      "intent_loss:  tensor(3.1092, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "\n",
    "# torch自带的sampler类，功能是每次返回一个随机的样本索引\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "# 使用dataloader输出batch\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "device = \"cpu\"\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "    if step > 1:\n",
    "        continue\n",
    "    \n",
    "    batch = tuple(t.to(device) for t in batch) # 将batch上传到显卡\n",
    "    inputs = {\"input_ids\": batch[0],\n",
    "              \"attention_mask\": batch[1],\n",
    "              \"token_type_ids\": batch[2],\n",
    "              \"intent_label_ids\": batch[3],}\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    print(\"input_ids: \", input_ids)\n",
    "    \n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "    intent_label_ids = inputs[\"intent_label_ids\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    outputs = model.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "    \n",
    "    pooled_output = outputs[1]  # [CLS]  [4 * 128]\n",
    "    intent_logits = model.intent_classifier(pooled_output)\n",
    "    print(\"intent_logits: \", intent_logits)   # [4 * 22]\n",
    "    print(\"intent_logits: \", intent_logits.shape)\n",
    "    \n",
    "    intent_loss_fct = nn.CrossEntropyLoss()\n",
    "    intent_loss = intent_loss_fct(intent_logits.view(-1, model.num_intent_labels), intent_label_ids.view(-1))\n",
    "    print(\"intent_loss: \", intent_loss)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
